
# model.py

"""
model.py
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
FrozenFeatureNet: ê³ ì •ëœ CNN ì¸ì½”ë” + í•™ìŠµ ê°€ëŠ¥í•œ ì„ë² ë”© + ì´ì§„ ë¶„ë¥˜ í—¤ë“œ.

- ì¶œë ¥ì€ **logit**(ì‹œê·¸ëª¨ì´ë“œ ë¯¸ì ìš©) â†’ í•™ìŠµ ì‹œ `nn.BCEWithLogitsLoss` ì‚¬ìš©
- ì¶”ë¡  ì‹œ `torch.sigmoid(output)` ë¡œ 0~1 í™•ë¥  ë³€í™˜
"""

import time
start_time = time.perf_counter()

import torch
import torch.nn as nn


class FrozenFeatureNet(nn.Module):
    def __init__(self, embedding_dim: int = 128):
        super().__init__()

        # ğŸ”’ 1. ê³ ì •(Frozen) CNN íŠ¹ì§• ì¶”ì¶œê¸°
        self.encoder = nn.Sequential(
            nn.Conv2d(3, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),  # â†’ [B, 32, 64, 64]
            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2), # â†’ [B, 64, 32, 32]
            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(),
            nn.AdaptiveAvgPool2d((1, 1))                                 # â†’ [B, 128, 1, 1]
        )
        for p in self.encoder.parameters():
            p.requires_grad = False  # ë°±ë³¸ì€ ì—…ë°ì´íŠ¸ X

        # 2. í•™ìŠµ ê°€ëŠ¥í•œ ì„ë² ë”© ë³€í™˜
        self.embedding = nn.Sequential(
            nn.Flatten(),                        # â†’ [B, 128]
            nn.Linear(128, embedding_dim), nn.ReLU()
        )

        # 3. ì´ì§„ ë¶„ë¥˜ í—¤ë“œ (logit 1ê°œ)
        self.classifier = nn.Linear(embedding_dim, 1)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ê¸°ë³¸ forward: logit ë°˜í™˜ â”€ í•™ìŠµÂ·ì¶”ë¡  ê³µìš©
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.encoder(x)
        x = self.embedding(x)
        return self.classifier(x)               # [B, 1] logit

    # ì„ë² ë”©ë§Œ ë½‘ê³  ì‹¶ì„ ë•Œ
    def forward_embed(self, x: torch.Tensor) -> torch.Tensor:
        x = self.encoder(x)
        return self.embedding(x)                # [B, embedding_dim]


# ë¹ ë¥¸ ë™ì‘ í™•ì¸ìš©
if __name__ == "__main__":
    model = FrozenFeatureNet()
    dummy = torch.randn(4, 3, 128, 128)        # 4ì¥ì˜ 128Ã—128 RGB ì´ë¯¸ì§€
    logits = model(dummy)
    print("Output shape:", logits.shape)        # [4, 1]
    trainable = sum(p.numel() for p in model.parameters()
                    if p.requires_grad)
    print(f"Trainable params: {trainable:,}")

    end_time = time.perf_counter()
    print(f"Loaded in {end_time - start_time:.2f} s")



'''
# model.py (ResNet18 ê¸°ë°˜ feature extractor + ê°œì„ ëœ classifier + triplet ì§€ì› + ê°•ì œ ì´ˆê¸°í™”)

import time
start_time = time.perf_counter()

import torch
import torch.nn as nn
import torchvision.models as models

class FrozenFeatureNet(nn.Module):
    def __init__(self, embedding_dim=128):
        super().__init__()

        # âœ… ResNet18 ì‚¬ì „í•™ìŠµ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
        backbone = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)
        self.encoder = nn.Sequential(*list(backbone.children())[:-1])  # [B, 512, 1, 1]

        # ğŸ”“ encoder í•™ìŠµ í—ˆìš© (fine-tuning)
        for param in self.encoder.parameters():
            param.requires_grad = True

        # âœ… embedding projection
        self.embedding = nn.Sequential(
            nn.Flatten(),                    # [B, 512]
            nn.Linear(512, embedding_dim),   # [B, embedding_dim]
            nn.ReLU()
        )

        # âœ… ê°œì„ ëœ ì´ì§„ ë¶„ë¥˜ í—¤ë“œ
        self.classifier = nn.Sequential(
            nn.Linear(embedding_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )

        # âœ… classifier weight ê°•ì œ ì´ˆê¸°í™”
        self.classifier.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            nn.init.kaiming_uniform_(m.weight, a=0.1)
            nn.init.constant_(m.bias, 0)

    def forward(self, x):
        x = self.encoder(x)
        x = self.embedding(x)
        return self.classifier(x)  # [B, 1]

    def forward_embed(self, x):
        x = self.encoder(x)
        return self.embedding(x)  # [B, embedding_dim]

    def forward_triplet(self, anchor, positive, negative):
        a = self.forward_embed(anchor)
        p = self.forward_embed(positive)
        n = self.forward_embed(negative)
        return a, p, n

end_time = time.perf_counter()
print(f"Total time: {end_time - start_time:.2f} s")
'''
